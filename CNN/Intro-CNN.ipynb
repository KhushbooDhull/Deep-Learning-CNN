{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbb706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d33b13",
   "metadata": {},
   "source": [
    "### CNN (Convolutional Neural Network)\n",
    "\n",
    "#### Why CNN when we can use ANN for classification?\n",
    "1) As ANNs dont scale well to image data.<br>\n",
    "2) Image data contains patterns <br>\n",
    "a) eyes, nose,ears for a human<br>\n",
    "b) lights, doors,mirror etc for a car<br>\n",
    "\n",
    "ANNs fail to recognize these patterns in the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bd35d",
   "metadata": {},
   "source": [
    "### CNN\n",
    "1) It is a class of NN used for Image Classification.<br>\n",
    "2) CNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ). Eg., An image of 6 x 6 x 3 array of matrix of RGB (3 refers to RGB values) and an image of 4 x 4 x 1 array of matrix of grayscale image.<br>\n",
    "3) Bottom line is that the role of the ConvNet is to reduce the images into a form that is easier to process, without losing features that are critical for getting a good prediction.<br>\n",
    "4) By using a CNN, one can enable sight to computers.<br>\n",
    "5) In CNN we have 4 layers\n",
    "\n",
    "    a) Convolution + Activation layer, \n",
    "    b) Pooling layer\n",
    "    c) Flatten layer \n",
    "    d) Dense layer \n",
    " \n",
    "The layers are arranged in such a way so that they detect simpler patterns first (lines, curves, etc.) and more complex patterns (faces, objects, etc.) further along.<br>\n",
    "\n",
    "6) In generel, there will be multiple (Convolution + Pooling) layers in a CNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7653b0",
   "metadata": {},
   "source": [
    "### Acrhitecture of CNN\n",
    "<img src=\"cnn_arch1.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10adef27",
   "metadata": {},
   "source": [
    "### Layers in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79254124",
   "metadata": {},
   "source": [
    "### 1) Colvolution Layer + Activation (relu)\n",
    "\n",
    "#### i) Convolution\n",
    "a) Convolution is the first layer to extract features from an input image. Convolution preserves the relationship between pixels by learning image features.<br>\n",
    "b) In this layer, the kernel slides across the height and width of the image-producing the image representation of that receptive region. This produces a two-dimensional representation of the image known as an feature map that gives the response of the kernel at each spatial position of the image. The sliding size of the kernel is called a stride.<br>\n",
    "c) On each image N different F * F filters(or kernels) are applied to extract features from the images<br> \n",
    "\n",
    "<img src=\"conv1.png\">\n",
    "<img src=\"conv2.png\">\n",
    "<img src=\"convolution_gif.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5cc85",
   "metadata": {},
   "source": [
    "### Mathematically\n",
    "\n",
    "<img src=\"conv3.png\" height=\"500\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82f757",
   "metadata": {},
   "source": [
    "### Shape of Feature Map\n",
    "\n",
    "Image= (h* w * d)<br>\n",
    "Filter=(fh*fw)<br>\n",
    "Stride=s<br>\n",
    "Padding= p=1\n",
    "\n",
    "#### Without Padding\n",
    "Feature Map Shape=[(h-fh)/s+1,(w-fw)/s+1)]\n",
    "\n",
    "#### With Padding\n",
    "Feature Map Shape= [(h-fh+2p)/s+1,(w-fw+2p)/s+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f3419",
   "metadata": {},
   "source": [
    "#### ii) Activation\n",
    "\n",
    "a) On the Feature map obtained from convolution, activation Function (like relu) is applied. \n",
    "<img src=\"conv_activation.png\">\n",
    "\n",
    "b) The first  convolutional layer usually extracts basic features such as horizontal or diagonal edges. This output is passed on to the next layer which detects more complex features such as corners or combinational edges. As we move deeper into the network it can identify even more complex features such as objects, faces, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02061c0f",
   "metadata": {},
   "source": [
    "### 2) Pooling Layer\n",
    "\n",
    "1) Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data by reducing the dimensions<br>\n",
    "2) This helps in reducing the spatial size of the representation, which decreases the required amount of computation and weights.<br>\n",
    "3) There are usually 2 types of Pooling - MaxPooling and AveragePooling.\n",
    "Sliding a window, we only take the maximum value inside the box on the left case. This is ‘max pooling.’ We can also take the average values like the picture on the right. This is ‘average pooling.’ And we can also tune the stride like what we do at the convolution layer.<br>\n",
    "Example of Pooling layer with a stride of 2<br>\n",
    "<img src=\"pooling2.jpg\" height=\"400\" width=\"350\">\n",
    "\n",
    "4)\tOn the feature map obtained after Convolutional + Activation layer, MaxPooling layer is applied to select the maximum value from that window of given stride. <br>\n",
    "In this example stride=1 <br>\n",
    "<img src=\"pooling1.png\">\n",
    "\n",
    "5)\tPooling also prevents overfitting as there are less parameters.<br>\n",
    "6) <b>Convolution + Pooling helps detect Translation Invariant features</b><br>\n",
    "\n",
    "Example of Translation Invariance<br>\n",
    "<img src=\"translation_invariance.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3d7a2",
   "metadata": {},
   "source": [
    "### 3) Flatten Layer\n",
    "\n",
    "1) Flattening is used to convert all the resultant 2-Dimensional arrays from pooled feature maps into a single long continuous linear vector.<br>\n",
    "2) Here we flatten the result of Conv + Pooling obtained from the last layer.<br>\n",
    "3) The Flattern layer doesn’t learn anything, and thus the number of parameters is 0. <br>\n",
    "4) We are making a classification model, which means these processed data should be good input to the model. It needs to be in the form of a 1-dimensional linear vector. Rectangular or cubic shapes can’t be direct inputs.<br>\n",
    "\n",
    "\n",
    "### 4) Dense Layer\n",
    "1) These layers are used to perform the classification the same way as ANN<br>\n",
    "\n",
    "<img src=\"flattening_dense.png\" height=\"400\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303b412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12367b1e",
   "metadata": {},
   "source": [
    "### Stride\n",
    "1) Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on.\n",
    "\n",
    "<img src=\"stride.jpg\" width=\"450\">\n",
    "\n",
    "### Padding\n",
    "1) It can be same or valid. <br>\n",
    "2) valid padding imples no padding. <br>\n",
    "3) same padding results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.<br>\n",
    "4) Padding is done so that corner and border pixels get their due weightage in terms of their participation in the convolution process.<br>\n",
    "\n",
    "<img src=\"padding1.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92429e7c",
   "metadata": {},
   "source": [
    "### Sample Architectures of CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3650ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,MaxPooling2D,AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d759e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f9da7",
   "metadata": {},
   "source": [
    "### Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db31ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=Sequential()\n",
    "m1.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)))#stride=1\n",
    "m1.add(MaxPooling2D(pool_size=(2,2))) #stride=2\n",
    "\n",
    "m1.add(Conv2D(32,(3,3),activation='relu'))\n",
    "m1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "m1.add(Flatten())\n",
    "m1.add(Dense(16,activation='relu'))\n",
    "m1.add(Dense(10,activation='softmax'))\n",
    "\n",
    "m1.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c31653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 32,090\n",
      "Trainable params: 32,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1.summary()\n",
    "# conv2D => 64*(1*(3*3)) + 64 = 640 \n",
    "# conv2D_1 => 32*(64*(3*3)) + 32 = 18464 \n",
    "# dense => 16*800 + 16 = 12816 \n",
    "# dense_1 => 10*16 + 10 = 170\n",
    "#Total= 640+18464+12816+170=32090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79779d",
   "metadata": {},
   "source": [
    "### Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d6c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=Sequential()\n",
    "m2.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1),padding='same'))\n",
    "m2.add(MaxPooling2D(pool_size=(2,2))) #stride=2\n",
    "\n",
    "m2.add(Conv2D(32,(3,3),activation='relu'))\n",
    "m2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "m2.add(Flatten())\n",
    "m2.add(Dense(16,activation='relu'))\n",
    "m2.add(Dense(10,activation='softmax'))\n",
    "\n",
    "m2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e4c152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                18448     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 37,722\n",
      "Trainable params: 37,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c3fda",
   "metadata": {},
   "source": [
    "### Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53600d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3=Sequential()\n",
    "m3.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1),strides=2))\n",
    "m3.add(MaxPooling2D(pool_size=(2,2))) #stride=2\n",
    "\n",
    "m3.add(Conv2D(32,(3,3),activation='relu'))\n",
    "m3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "m3.add(Flatten())\n",
    "m3.add(Dense(16,activation='relu'))\n",
    "m3.add(Dense(10,activation='softmax'))\n",
    "\n",
    "m3.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2a9b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 32)          18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 21,338\n",
      "Trainable params: 21,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf3b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981a7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
